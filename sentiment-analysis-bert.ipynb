{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ef99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_corpus, stopwords, processing, processing_bert\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da01ef",
   "metadata": {
    "cell_id": 39
   },
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef093bd4",
   "metadata": {
    "cell_id": 1
   },
   "outputs": [],
   "source": [
    "today = datetime.date.today().strftime('%Y%m%d')\n",
    "if not os.path.exists('./evaluation-bert'):\n",
    "    os.makedirs('./evaluation-bert')\n",
    "writer = SummaryWriter(log_dir=os.path.join('./evaluation-bert', today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c97699",
   "metadata": {
    "cell_id": 3
   },
   "outputs": [],
   "source": [
    "# 分别加载训练集和测试集\n",
    "df_train=pd.read_csv('../Dataset/weibo_senti_bert_train.csv') \n",
    "df_test=pd.read_csv('../Dataset/weibo_senti_bert_test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b192907f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/evaluation-bert/20220218'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('/evaluation-bert', today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cceedf1",
   "metadata": {
    "cell_id": 41
   },
   "source": [
    "### 加载Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed1dd7c",
   "metadata": {
    "cell_id": 5
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"    # 在我的电脑上不加这一句, bert模型会报错\n",
    "MODEL_PATH = \"../WeiboSentiment/model/chinese_wwm_pytorch\"     # 下载地址见 https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eca406",
   "metadata": {
    "cell_id": 43
   },
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caab7716",
   "metadata": {
    "cell_id": 7
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f444ad",
   "metadata": {
    "cell_id": 6
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../WeiboSentiment/model/chinese_wwm_pytorch were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 加载\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)   # 分词器\n",
    "bert = BertModel.from_pretrained(MODEL_PATH).to(device)            # 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f96f232",
   "metadata": {
    "cell_id": 8
   },
   "outputs": [],
   "source": [
    "# 超参数\n",
    "learning_rate = 1e-3\n",
    "input_size = 768\n",
    "num_epoches = 30\n",
    "batch_size = 100\n",
    "decay_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe62471",
   "metadata": {
    "cell_id": 9
   },
   "outputs": [],
   "source": [
    "# 数据集\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df[\"text\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "# 训练集\n",
    "train_data = MyDataset(df_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 测试集\n",
    "test_data = MyDataset(df_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6026a02",
   "metadata": {
    "cell_id": 10
   },
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dda16d1",
   "metadata": {
    "cell_id": 34
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# 测试集效果检验\n",
    "def test():\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, labels in test_loader:\n",
    "            tokens = tokenizer(words, padding=True)\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0][:, 0]\n",
    "            outputs = net(bert_output)          # 前向传播\n",
    "            outputs = outputs.view(-1)          # 将输出展平\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(labels)\n",
    "\n",
    "    y_prob = torch.cat(y_pred).cpu()\n",
    "    y_true = torch.cat(y_true).cpu()\n",
    "    y_pred = y_prob.clone()\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    \n",
    "    # print(metrics.classification_report(y_true, y_pred))\n",
    "    # print(\"准确率:\", metrics.accuracy_score(y_true, y_pred))\n",
    "    # print(\"AUC:\", metrics.roc_auc_score(y_true, y_prob) )\n",
    "    precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_true, y_prob)\n",
    "    print('Epoch {}/{}, P {:.4f}, R {:.4f}, F1 {:.4f}, AUC {:.4f}'.format(\n",
    "        epoch, num_epoches, precision.mean(), recall.mean(), f1.mean(), roc_auc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287e1b12",
   "metadata": {
    "cell_id": 11
   },
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c454c245",
   "metadata": {
    "cell_id": 14,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, step:500, loss:18.69440269470215\n",
      "epoch:1, step:1000, loss:15.497702598571777\n",
      "epoch:1, step:1500, loss:14.89936351776123\n",
      "Epoch 0/30, P 0.8812, R 0.8812, F1 0.8812, AUC 0.9544\n",
      "saved model:  ./model/classification/bert_dnn_1.model\n",
      "epoch:2, step:500, loss:14.426989555358887\n",
      "epoch:2, step:1000, loss:14.13404369354248\n",
      "epoch:2, step:1500, loss:14.374293327331543\n",
      "Epoch 1/30, P 0.8876, R 0.8855, F1 0.8857, AUC 0.9569\n",
      "saved model:  ./model/classification/bert_dnn_2.model\n",
      "epoch:3, step:500, loss:14.180218696594238\n",
      "epoch:3, step:1000, loss:13.816229820251465\n",
      "epoch:3, step:1500, loss:13.99055004119873\n",
      "Epoch 2/30, P 0.8877, R 0.8868, F1 0.8869, AUC 0.9582\n",
      "saved model:  ./model/classification/bert_dnn_3.model\n",
      "epoch:4, step:500, loss:13.778979301452637\n",
      "epoch:4, step:1000, loss:13.76666259765625\n",
      "epoch:4, step:1500, loss:13.744775772094727\n",
      "Epoch 3/30, P 0.8892, R 0.8845, F1 0.8847, AUC 0.9589\n",
      "saved model:  ./model/classification/bert_dnn_4.model\n",
      "epoch:5, step:500, loss:13.810759544372559\n",
      "epoch:5, step:1000, loss:13.65839672088623\n",
      "epoch:5, step:1500, loss:13.838194847106934\n",
      "Epoch 4/30, P 0.8894, R 0.8863, F1 0.8865, AUC 0.9592\n",
      "saved model:  ./model/classification/bert_dnn_5.model\n",
      "epoch:6, step:500, loss:13.534814834594727\n",
      "epoch:6, step:1000, loss:13.513511657714844\n",
      "epoch:6, step:1500, loss:13.788322448730469\n",
      "Epoch 5/30, P 0.8890, R 0.8880, F1 0.8882, AUC 0.9593\n",
      "saved model:  ./model/classification/bert_dnn_6.model\n",
      "epoch:7, step:500, loss:13.65363597869873\n",
      "epoch:7, step:1000, loss:13.479199409484863\n",
      "epoch:7, step:1500, loss:13.64776611328125\n",
      "Epoch 6/30, P 0.8898, R 0.8883, F1 0.8885, AUC 0.9596\n",
      "saved model:  ./model/classification/bert_dnn_7.model\n",
      "epoch:8, step:500, loss:13.5278902053833\n",
      "epoch:8, step:1000, loss:13.509404182434082\n",
      "epoch:8, step:1500, loss:13.51341724395752\n",
      "Epoch 7/30, P 0.8892, R 0.8881, F1 0.8883, AUC 0.9597\n",
      "saved model:  ./model/classification/bert_dnn_8.model\n",
      "epoch:9, step:500, loss:13.418822288513184\n",
      "epoch:9, step:1000, loss:13.843185424804688\n",
      "epoch:9, step:1500, loss:13.488624572753906\n",
      "Epoch 8/30, P 0.8871, R 0.8871, F1 0.8871, AUC 0.9598\n",
      "saved model:  ./model/classification/bert_dnn_9.model\n",
      "epoch:10, step:500, loss:13.292253494262695\n",
      "epoch:10, step:1000, loss:13.501795768737793\n",
      "epoch:10, step:1500, loss:13.395380020141602\n",
      "Epoch 9/30, P 0.8874, R 0.8865, F1 0.8866, AUC 0.9599\n",
      "saved model:  ./model/classification/bert_dnn_10.model\n",
      "epoch:11, step:500, loss:13.391426086425781\n",
      "epoch:11, step:1000, loss:13.421195983886719\n",
      "epoch:11, step:1500, loss:13.368481636047363\n",
      "Epoch 10/30, P 0.8894, R 0.8871, F1 0.8873, AUC 0.9600\n",
      "saved model:  ./model/classification/bert_dnn_11.model\n",
      "epoch:12, step:500, loss:13.545814514160156\n",
      "epoch:12, step:1000, loss:13.291165351867676\n",
      "epoch:12, step:1500, loss:13.371042251586914\n",
      "Epoch 11/30, P 0.8894, R 0.8879, F1 0.8880, AUC 0.9601\n",
      "saved model:  ./model/classification/bert_dnn_12.model\n",
      "epoch:13, step:500, loss:13.349023818969727\n",
      "epoch:13, step:1000, loss:13.42292308807373\n",
      "epoch:13, step:1500, loss:13.398825645446777\n",
      "Epoch 12/30, P 0.8899, R 0.8878, F1 0.8879, AUC 0.9602\n",
      "saved model:  ./model/classification/bert_dnn_13.model\n",
      "epoch:14, step:500, loss:13.315741539001465\n",
      "epoch:14, step:1000, loss:13.465168952941895\n",
      "epoch:14, step:1500, loss:13.335583686828613\n",
      "Epoch 13/30, P 0.8891, R 0.8870, F1 0.8872, AUC 0.9603\n",
      "saved model:  ./model/classification/bert_dnn_14.model\n",
      "epoch:15, step:500, loss:13.313054084777832\n",
      "epoch:15, step:1000, loss:13.394316673278809\n",
      "epoch:15, step:1500, loss:13.343263626098633\n",
      "Epoch 14/30, P 0.8892, R 0.8888, F1 0.8888, AUC 0.9603\n",
      "saved model:  ./model/classification/bert_dnn_15.model\n",
      "epoch:16, step:500, loss:13.287304878234863\n",
      "epoch:16, step:1000, loss:13.410853385925293\n",
      "epoch:16, step:1500, loss:13.299280166625977\n",
      "Epoch 15/30, P 0.8896, R 0.8889, F1 0.8890, AUC 0.9603\n",
      "saved model:  ./model/classification/bert_dnn_16.model\n",
      "epoch:17, step:500, loss:13.460888862609863\n",
      "epoch:17, step:1000, loss:13.322687149047852\n",
      "epoch:17, step:1500, loss:13.249427795410156\n",
      "Epoch 16/30, P 0.8893, R 0.8889, F1 0.8889, AUC 0.9603\n",
      "saved model:  ./model/classification/bert_dnn_17.model\n",
      "epoch:18, step:500, loss:13.52377986907959\n",
      "epoch:18, step:1000, loss:13.173453330993652\n",
      "epoch:18, step:1500, loss:13.356945991516113\n",
      "Epoch 17/30, P 0.8897, R 0.8892, F1 0.8893, AUC 0.9604\n",
      "saved model:  ./model/classification/bert_dnn_18.model\n",
      "epoch:19, step:500, loss:13.256586074829102\n",
      "epoch:19, step:1000, loss:13.215998649597168\n",
      "epoch:19, step:1500, loss:13.564155578613281\n",
      "Epoch 18/30, P 0.8897, R 0.8892, F1 0.8893, AUC 0.9604\n",
      "saved model:  ./model/classification/bert_dnn_19.model\n",
      "epoch:20, step:500, loss:13.221203804016113\n",
      "epoch:20, step:1000, loss:13.441184043884277\n",
      "epoch:20, step:1500, loss:13.367965698242188\n",
      "Epoch 19/30, P 0.8897, R 0.8889, F1 0.8890, AUC 0.9604\n",
      "saved model:  ./model/classification/bert_dnn_20.model\n",
      "epoch:21, step:500, loss:13.359599113464355\n",
      "epoch:21, step:1000, loss:13.291651725769043\n",
      "epoch:21, step:1500, loss:13.202574729919434\n",
      "Epoch 20/30, P 0.8898, R 0.8888, F1 0.8890, AUC 0.9604\n",
      "saved model:  ./model/classification/bert_dnn_21.model\n",
      "epoch:22, step:500, loss:13.232701301574707\n",
      "epoch:22, step:1000, loss:13.478171348571777\n",
      "epoch:22, step:1500, loss:13.334197998046875\n",
      "Epoch 21/30, P 0.8897, R 0.8892, F1 0.8893, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_22.model\n",
      "epoch:23, step:500, loss:13.409472465515137\n",
      "epoch:23, step:1000, loss:13.384068489074707\n",
      "epoch:23, step:1500, loss:13.228894233703613\n",
      "Epoch 22/30, P 0.8898, R 0.8878, F1 0.8880, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_23.model\n",
      "epoch:24, step:500, loss:13.367091178894043\n",
      "epoch:24, step:1000, loss:13.311357498168945\n",
      "epoch:24, step:1500, loss:13.231264114379883\n",
      "Epoch 23/30, P 0.8900, R 0.8893, F1 0.8894, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_24.model\n",
      "epoch:25, step:500, loss:13.162620544433594\n",
      "epoch:25, step:1000, loss:13.33036994934082\n",
      "epoch:25, step:1500, loss:13.519645690917969\n",
      "Epoch 24/30, P 0.8897, R 0.8870, F1 0.8872, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_25.model\n",
      "epoch:26, step:500, loss:13.210474014282227\n",
      "epoch:26, step:1000, loss:13.283716201782227\n",
      "epoch:26, step:1500, loss:13.425871849060059\n",
      "Epoch 25/30, P 0.8900, R 0.8890, F1 0.8891, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_26.model\n",
      "epoch:27, step:500, loss:13.289044380187988\n",
      "epoch:27, step:1000, loss:13.396430969238281\n",
      "epoch:27, step:1500, loss:13.217429161071777\n",
      "Epoch 26/30, P 0.8902, R 0.8891, F1 0.8893, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_27.model\n",
      "epoch:28, step:500, loss:13.171004295349121\n",
      "epoch:28, step:1000, loss:13.32933521270752\n",
      "epoch:28, step:1500, loss:13.352670669555664\n",
      "Epoch 27/30, P 0.8903, R 0.8892, F1 0.8893, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_28.model\n",
      "epoch:29, step:500, loss:13.302958488464355\n",
      "epoch:29, step:1000, loss:13.416167259216309\n",
      "epoch:29, step:1500, loss:13.285913467407227\n",
      "Epoch 28/30, P 0.8901, R 0.8893, F1 0.8894, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_29.model\n",
      "epoch:30, step:500, loss:13.291739463806152\n",
      "epoch:30, step:1000, loss:13.324856758117676\n",
      "epoch:30, step:1500, loss:13.160392761230469\n",
      "Epoch 29/30, P 0.8900, R 0.8890, F1 0.8892, AUC 0.9605\n",
      "saved model:  ./model/classification/bert_dnn_30.model\n"
     ]
    }
   ],
   "source": [
    "# 迭代训练\n",
    "for epoch in range(num_epoches):\n",
    "    total_loss = 0\n",
    "    for i, (words, labels) in enumerate(train_loader):\n",
    "        tokens = tokenizer(words, padding=True)\n",
    "        input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "            bert_output = last_hidden_states[0][:, 0]\n",
    "        optimizer.zero_grad()               # 梯度清零\n",
    "        outputs = net(bert_output)          # 前向传播\n",
    "        logits = outputs.view(-1)           # 将输出展平\n",
    "        loss = criterion(logits, labels)    # loss计算\n",
    "        total_loss += loss\n",
    "        loss.backward()                     # 反向传播，计算梯度\n",
    "        optimizer.step()                    # 梯度更新\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(\"epoch:{}, step:{}, loss:{}\".format(epoch+1, i+1, total_loss/10))\n",
    "            total_loss = 0\n",
    "    \n",
    "    # learning_rate decay\n",
    "    scheduler.step()\n",
    "    \n",
    "    # test\n",
    "    test()\n",
    "    \n",
    "    # save model\n",
    "    model_path = \"./model/classification/bert_dnn_{}.model\".format(epoch+1)\n",
    "    torch.save(net, model_path)\n",
    "    print(\"saved model: \", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0687ba18",
   "metadata": {
    "cell_id": 23
   },
   "source": [
    "### 手动输入句子，判断情感倾向（1正/0负）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41570385",
   "metadata": {
    "cell_id": 38
   },
   "outputs": [],
   "source": [
    "net = torch.load(\"./model/classification/bert_dnn_8.model\")    # 训练过程中的巅峰时刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fcd5f39",
   "metadata": {
    "cell_id": 37
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6120],\n",
       "        [0.1450]], device='cuda:1', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"华丽繁荣的城市、充满回忆的小镇、郁郁葱葱的山谷...\", \"突然就觉得人间不值得\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ec5e421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6120356917381287"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.view(-1).cpu()[0].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a234d6",
   "metadata": {
    "cell_id": 27,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9864],\n",
       "        [0.9264]], device='cuda:1', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"今天天气真好\", \"今天天气特别特别棒\"]\n",
    "tokens = tokenizer(s, padding=True)\n",
    "input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "bert_output = last_hidden_states[0][:, 0]\n",
    "outputs = net(bert_output)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abb74e",
   "metadata": {},
   "source": [
    "## 评估（inference）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e37b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from utils import processing,processing_bert\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from gensim import models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5149f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60bfc10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/nas/WeiboSentiment/model/chinese_wwm_pytorch were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish label emotion score through bert model!!\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"   \n",
    "MODEL_PATH = '/root/nas/WeiboSentiment/model/chinese_wwm_pytorch'     # 下载地址见 https://github.com/ymcui/Chinese-BERT-wwm\n",
    "# 加载\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)   # 分词器\n",
    "bert = BertModel.from_pretrained(MODEL_PATH).to(device)            # 模型\n",
    "# 超参数\n",
    "input_size = 768\n",
    "net = Net(input_size)\n",
    "net = torch.load('/root/nas/chinese-sentiment-analysis/model/classification/bert_dnn_20.model')    # 训练过程中的巅峰时刻\n",
    "net = net.to(device)\n",
    "\n",
    "def calculate_emo_score_bert(text):\n",
    "    text = processing_bert(text)\n",
    "    if len(text)>510:\n",
    "        text=text[:510]\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer([text], padding=True)\n",
    "        input_ids = torch.tensor(tokens[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).to(device)\n",
    "        last_hidden_states = bert(input_ids, attention_mask=attention_mask)\n",
    "        bert_output = last_hidden_states[0][:, 0]\n",
    "        outputs = net(bert_output)\n",
    "        result_score = outputs.view(-1).cpu()[0].item() \n",
    "    return result_score\n",
    "\n",
    "    # 模型结果\n",
    "    # 读取爬虫数据\n",
    "crawl_result = pd.read_csv('/root/nas/Dataset/crawl_result.csv')\n",
    "crawl_result['emo_score'] = crawl_result['texts'].apply(lambda x: calculate_emo_score_bert(x))\n",
    "crawl_result.to_csv('crawl_result_emo_bert.csv', index=False, encoding='utf-8-sig')\n",
    "print('finish label emotion score through bert model!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    " --output_path /root/nas/chinese-sentiment-analysis/result/crawl_result_emo_bert.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "max_cell_id": 45
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
